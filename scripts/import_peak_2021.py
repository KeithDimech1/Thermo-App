#!/usr/bin/env python3
"""
Import Peak et al. (2021) Grand Canyon ZHe data into database
Uses EarthBank-compatible CSV files generated by extraction workflow
"""

import pandas as pd
import psycopg2
from psycopg2.extras import execute_values
from pathlib import Path
import sys
from dotenv import load_dotenv
import os

# Load environment variables
env_path = Path(__file__).parent.parent / '.env.local'
load_dotenv(env_path)

DATABASE_URL = os.getenv('DATABASE_URL')

if not DATABASE_URL:
    print("ERROR: DATABASE_URL not set in .env.local")
    sys.exit(1)

def import_peak_2021():
    """Import Peak et al. (2021) data into database"""

    # File paths
    base_dir = Path(__file__).parent.parent / "build-data/learning/thermo-papers/Peak(2021)-Grand-Canyon-Great-Unconformity-Geology/FAIR"

    samples_file = base_dir / "earthbank_samples.csv"
    datapoints_file = base_dir / "earthbank_he_datapoints.csv"
    grains_file = base_dir / "earthbank_he_whole_grain_data.csv"

    # Verify files exist
    for file in [samples_file, datapoints_file, grains_file]:
        if not file.exists():
            print(f"ERROR: File not found: {file}")
            sys.exit(1)

    # Load CSV files
    print("Loading CSV files...")
    df_samples = pd.read_csv(samples_file)
    df_datapoints = pd.read_csv(datapoints_file)
    df_grains = pd.read_csv(grains_file)

    print(f"  Samples: {len(df_samples)}")
    print(f"  Datapoints: {len(df_datapoints)}")
    print(f"  Grains: {len(df_grains)}")

    # Connect to database
    print("\nConnecting to database...")
    print(f"Database URL: {DATABASE_URL[:50]}...")
    conn = psycopg2.connect(DATABASE_URL)
    cur = conn.cursor()

    # Verify connection
    cur.execute("SELECT current_database(), count(*) FROM datasets;")
    dbname, dataset_count = cur.fetchone()
    print(f"Connected to database: {dbname}")
    print(f"Existing datasets: {dataset_count}")

    try:
        # Step 1: Get or create dataset
        print("\n" + "="*80)
        print("STEP 1: Getting dataset...")
        print("="*80)

        # Get first available dataset
        cur.execute("SELECT id, dataset_name FROM datasets LIMIT 1;")
        result = cur.fetchone()
        if result:
            dataset_id = result[0]
            print(f"✅ Using existing dataset: {result[1]} (ID: {dataset_id})")
        else:
            print("No datasets found, creating one...")
            cur.execute("""
                INSERT INTO datasets (dataset_name, description, doi, study_area)
                VALUES (%s, %s, %s, %s)
                RETURNING id;
            """, ('Peak et al. (2021)', 'Grand Canyon ZHe', '10.1130/G49116.1', 'Grand Canyon, AZ'))
            dataset_id = cur.fetchone()[0]
            print(f"✅ Created new dataset: ID {dataset_id}")

        # Step 2: Insert samples
        print("\n" + "="*80)
        print("STEP 2: Inserting samples...")
        print("="*80)

        # Prepare sample data with available metadata
        sample_data = []
        for _, row in df_samples.iterrows():
            # Note: Missing fields (IGSN, lat/lon, lithology) will be NULL
            sample_tuple = (
                row['Sample'],  # sample_id
                dataset_id,     # dataset_id
                row['IGSN'] if pd.notna(row['IGSN']) and row['IGSN'] != '' else None,
                'in situ rock',  # sample_kind
                row['Latitude'] if pd.notna(row['Latitude']) and row['Latitude'] != '' else None,
                row['Longitude'] if pd.notna(row['Longitude']) and row['Longitude'] != '' else None,
                'WGS84',  # geodetic_datum
                row['Elevation (m)'] if pd.notna(row['Elevation (m)']) and row['Elevation (m)'] != '' else None,
                'mean sea level',  # vertical_datum
                row['Lithology'] if pd.notna(row['Lithology']) and row['Lithology'] != '' else None,
                'zircon',  # mineral_type
            )
            sample_data.append(sample_tuple)

        sample_insert = """
        INSERT INTO samples (
            sample_id, dataset_id, igsn, sample_kind,
            latitude, longitude, geodetic_datum,
            elevation_m, vertical_datum,
            lithology, mineral_type
        ) VALUES %s
        ON CONFLICT (sample_id) DO UPDATE
        SET dataset_id = EXCLUDED.dataset_id,
            mineral_type = EXCLUDED.mineral_type;
        """

        execute_values(cur, sample_insert, sample_data)
        print(f"✅ Inserted {len(sample_data)} samples")

        # Step 3: Insert he_datapoints
        print("\n" + "="*80)
        print("STEP 3: Inserting he_datapoints...")
        print("="*80)

        datapoint_data = []
        datapoint_ids = {}  # Map datapoint_key to database ID

        for _, row in df_datapoints.iterrows():
            # Convert year to timestamp if it's just a year
            analysis_date = None
            if pd.notna(row['Analysis_Date']):
                try:
                    analysis_date = f"{int(row['Analysis_Date'])}-01-01"  # Convert year to date
                except:
                    analysis_date = str(row['Analysis_Date'])

            datapoint_tuple = (
                row['Sample'],  # sample_id
                row['Datapoint_ID'],  # datapoint_key
                row['Laboratory'] if pd.notna(row['Laboratory']) else None,
                row['Analyst'] if pd.notna(row['Analyst']) else None,
                analysis_date,
                'zircon',  # mineral_type
                int(row['Num_Grains']) if pd.notna(row['Num_Grains']) else None,
                float(row['Mean_Corrected_Age_Ma']) if pd.notna(row['Mean_Corrected_Age_Ma']) else None,
                float(row['Mean_Corrected_Age_1s_Ma']) if pd.notna(row['Mean_Corrected_Age_1s_Ma']) else None,
                '1s',  # error type
                float(row['Mean_Uncorrected_Age_Ma']) if pd.notna(row['Mean_Uncorrected_Age_Ma']) else None,
                row['Notes'] if pd.notna(row['Notes']) else None,
            )
            datapoint_data.append(datapoint_tuple)

        datapoint_insert = """
        INSERT INTO he_datapoints (
            sample_id, datapoint_key, laboratory, analyst_orcid, analysis_date,
            mineral_type, n_aliquots,
            mean_corr_age_ma, mean_corr_age_error_ma, mean_corr_age_error_type,
            mean_uncorr_age_ma,
            uncertainty_description
        ) VALUES %s
        ON CONFLICT (datapoint_key) DO UPDATE
        SET mean_corr_age_ma = EXCLUDED.mean_corr_age_ma
        RETURNING id, datapoint_key;
        """

        # Execute and capture returned IDs
        for datapoint in datapoint_data:
            cur.execute("""
                INSERT INTO he_datapoints (
                    sample_id, datapoint_key, laboratory, analyst_orcid, analysis_date,
                    mineral_type, n_aliquots,
                    mean_corr_age_ma, mean_corr_age_error_ma, mean_corr_age_error_type,
                    mean_uncorr_age_ma,
                    uncertainty_description
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (datapoint_key) DO UPDATE
                SET mean_corr_age_ma = EXCLUDED.mean_corr_age_ma
                RETURNING id, datapoint_key;
            """, datapoint)
            result = cur.fetchone()
            datapoint_ids[result[1]] = result[0]

        print(f"✅ Inserted {len(datapoint_data)} he_datapoints")

        # Step 4: Insert he_whole_grain_data
        print("\n" + "="*80)
        print("STEP 4: Inserting he_whole_grain_data...")
        print("="*80)

        grain_data = []
        for _, row in df_grains.iterrows():
            # Get he_datapoint_id from mapping
            datapoint_key = row['Datapoint_ID']
            if datapoint_key not in datapoint_ids:
                print(f"WARNING: Datapoint {datapoint_key} not found, skipping grain {row['Grain_ID']}")
                continue

            he_datapoint_id = datapoint_ids[datapoint_key]

            grain_tuple = (
                he_datapoint_id,  # he_datapoint_id
                row['Grain_ID'] if pd.notna(row['Grain_ID']) else None,  # lab_no
                row['Grain_ID'] if pd.notna(row['Grain_ID']) else None,  # grain_identifier
                'single',  # aliquot_type
                1,  # n_grains_in_aliquot
                float(row['Length_um']) if pd.notna(row['Length_um']) else None,  # length_um
                float(row['Width_um']) if pd.notna(row['Width_um']) else None,  # half_width_um
                float(row['He_ncc']) if pd.notna(row['He_ncc']) else None,  # He_ncc
                float(row['U_ppm']) if pd.notna(row['U_ppm']) else None,  # U_ppm
                float(row['U_ppm_1s']) if pd.notna(row['U_ppm_1s']) else None,  # U_ppm_error
                float(row['Th_ppm']) if pd.notna(row['Th_ppm']) else None,  # Th_ppm
                float(row['Th_ppm_1s']) if pd.notna(row['Th_ppm_1s']) else None,  # Th_ppm_error
                float(row['Sm_ppm']) if pd.notna(row['Sm_ppm']) else None,  # Sm_ppm
                float(row['Sm_ppm_1s']) if pd.notna(row['Sm_ppm_1s']) else None,  # Sm_ppm_error
                float(row['eU_ppm']) if pd.notna(row['eU_ppm']) else None,  # eU_ppm
                float(row['Mass_mg']) if pd.notna(row['Mass_mg']) else None,  # mass_mg
                float(row['Rs_um']) if pd.notna(row['Rs_um']) else None,  # Rs_um
                float(row['Raw_Age_Ma']) if pd.notna(row['Raw_Age_Ma']) else None,  # uncorr_age_ma
                float(row['Raw_Age_1s_Ma']) if pd.notna(row['Raw_Age_1s_Ma']) else None,  # uncorr_age_error_ma
                float(row['Corrected_Age_Ma']) if pd.notna(row['Corrected_Age_Ma']) else None,  # corr_age_ma
                float(row['Corrected_Age_1s_Ma']) if pd.notna(row['Corrected_Age_1s_Ma']) else None,  # corr_age_1sigma_ma
                float(row['FT']) if pd.notna(row['FT']) else None,  # FT
            )
            grain_data.append(grain_tuple)

        grain_insert = """
        INSERT INTO he_whole_grain_data (
            he_datapoint_id, lab_no, grain_identifier,
            aliquot_type, n_grains_in_aliquot,
            length_um, half_width_um,
            He_ncc, He_blank_error_ncc,
            U_ppm, U_ppm_error,
            Th_ppm, Th_ppm_error,
            Sm_ppm, Sm_ppm_error,
            eU_ppm,
            Rs_um,
            mass_mg,
            uncorr_age_ma, uncorr_age_error_ma,
            corr_age_ma, corr_age_1sigma_ma,
            FT
        ) VALUES %s
        ON CONFLICT (lab_no) DO UPDATE
        SET corr_age_ma = EXCLUDED.corr_age_ma;
        """

        # Simpler insert without ON CONFLICT for easier debugging
        for grain in grain_data:
            cur.execute("""
                INSERT INTO he_whole_grain_data (
                    he_datapoint_id, lab_no, grain_identifier,
                    aliquot_type, n_grains_in_aliquot,
                    length_um, half_width_um,
                    He_ncc,
                    U_ppm, U_ppm_error,
                    Th_ppm, Th_ppm_error,
                    Sm_ppm, Sm_ppm_error,
                    eU_ppm,
                    mass_mg,
                    Rs_um,
                    uncorr_age_ma, uncorr_age_error_ma,
                    corr_age_ma, corr_age_1sigma_ma,
                    FT
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (lab_no) DO UPDATE
                SET corr_age_ma = EXCLUDED.corr_age_ma;
            """, grain)

        print(f"✅ Inserted {len(grain_data)} he_whole_grain_data rows")

        # Commit transaction
        conn.commit()

        print("\n" + "="*80)
        print("✅ IMPORT COMPLETE!")
        print("="*80)

        # Print summary
        print("\nImport Summary:")
        print(f"  Dataset ID: {dataset_id}")
        print(f"  Samples: {len(sample_data)}")
        print(f"  He Datapoints: {len(datapoint_data)}")
        print(f"  He Grains: {len(grain_data)}")

        # Verify import
        print("\n" + "="*80)
        print("VERIFICATION")
        print("="*80)

        cur.execute("""
            SELECT
                s.sample_id,
                COUNT(DISTINCT hd.id) as n_datapoints,
                COUNT(hg.id) as n_grains,
                AVG(hg.corr_age_ma) as mean_age,
                MIN(hg.corr_age_ma) as min_age,
                MAX(hg.corr_age_ma) as max_age
            FROM samples s
            LEFT JOIN he_datapoints hd ON s.sample_id = hd.sample_id
            LEFT JOIN he_whole_grain_data hg ON hd.id = hg.he_datapoint_id
            WHERE s.dataset_id = %s
            GROUP BY s.sample_id
            ORDER BY s.sample_id;
        """, (dataset_id,))

        results = cur.fetchall()
        print(f"\n{'Sample':<15} {'Datapoints':<12} {'Grains':<10} {'Mean Age':<12} {'Range (Ma)'}")
        print("-" * 70)
        for row in results:
            sample_id, n_dp, n_gr, mean_age, min_age, max_age = row
            age_range = f"{min_age:.1f} - {max_age:.1f}" if min_age and max_age else "N/A"
            mean_str = f"{mean_age:.1f}" if mean_age else "N/A"
            print(f"{sample_id:<15} {n_dp:<12} {n_gr:<10} {mean_str:<12} {age_range}")

    except Exception as e:
        conn.rollback()
        print(f"\n❌ ERROR during import: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    finally:
        cur.close()
        conn.close()

if __name__ == "__main__":
    import_peak_2021()
